Copyright 1999, 2001 Free Software Foundation, Inc.

This file is part of the GNU MP Library.

The GNU MP Library is free software; you can redistribute it and/or modify
it under the terms of the GNU Lesser General Public License as published by
the Free Software Foundation; either version 2.1 of the License, or (at your
option) any later version.

The GNU MP Library is distributed in the hope that it will be useful, but
WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY
or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU Lesser General Public
License for more details.

You should have received a copy of the GNU Lesser General Public License
along with the GNU MP Library; see the file COPYING.LIB.  If not, write to
the Free Software Foundation, Inc., 59 Temple Place - Suite 330, Boston, MA
02111-1307, USA.




This directory contains mpn functions for 64-bit PA-RISC 2.0.

RELEVANT OPTIMIZATION ISSUES

The PA8x00 processors have a 4-way OOO pipeline.  No register RAW
latency scheduling is necessary.  Memory RAW scheduling is still
necessary.  Completion unit scheduling is necessary.

According to HP's published publication, two memory operations can be
completed per cycle, but at most one of these may be a store.  Two ALU
operations may be completed per cycle; integer and fp operations are
equal here.  Tests indicate that in practice when a store is executed,
no load can execute in the same cycle.

Since the completion unit can only handle 4 instructions per cycle,
and since completion is made in strict program order, we need to
schedule 4-group instructions that can complete together, e.g., two
loads and two ALU operations.

Instructions that read the carry flag seem to cause hickup in the
pipeline.

Warning for the "nop" mnemonic; it stalls the pipeline.

STATUS

* mpn_lshift and mpn_rshift run at 1.5 cycles/limb.  That cannot be
  improved.

* mpn_add_n and mpn_sub_n run at 2.0 cycles/limb.  That cannot be
  improved.

* The multiplication functions run at 11 cycles/limb.  The memory unit
  bandwidth allows 5.0 cycles/limb for mul_1 and 5.5 cycles/limb for
  addmul_1/submul_1.  With proper scheduling, the performance will be
  limited by ALU bandwidth.

* xaddmul_1.S contains a quicker method for forming the 128 bit product.  It
  uses some fewer operations, and keep the carry flag live across the loop
  boundary.  But it seems hard to make it run more than 1/4 cycle faster
  than the old code.  Perhaps we really ought to unroll this loop be 2x?  2x
  should suffice since register latency schedling is never needed, but the
  unrolling would hide the RAW memory latency.  Here is a sketch:

	1. A multiply and store 64-bit products
	2. B sum 64-bit products 128-bit product
	3. B load  64-bit products to integer registers
	4. B multiply and store 64-bit products
	5. A sum 64-bit products 128-bit product
	6. A load  64-bit products to integer registers
	7. goto 1

  In practice, adjacent groups (1 and 2, 2 and 3, etc) will be interleaved
  for better instruction mix.

 1                 
 2                 6                 0
 8                 4

sp[i] * x
  ________ ________
 |________|________|			phi
	   ________ ________
	  |________|________|		pm1
	   ________ ________
	  |________|________|		pm0
		    ________ ________
		   |________|________|	plo
		    ________ ________
		   |________|________|	rpi
		    ________ ________
		   |________|________|	cyl


(rpi >> B/2) + (plo >> B/2) + pm0 + pm1
