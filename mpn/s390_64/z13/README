Copyright 2023 Free Software Foundation, Inc.

This file is part of the GNU MP Library.

The GNU MP Library is free software; you can redistribute it and/or modify
it under the terms of either:

  * the GNU Lesser General Public License as published by the Free
    Software Foundation; either version 3 of the License, or (at your
    option) any later version.

or

  * the GNU General Public License as published by the Free Software
    Foundation; either version 2 of the License, or (at your option) any
    later version.

or both in parallel, as here.

The GNU MP Library is distributed in the hope that it will be useful, but
WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY
or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
for more details.

You should have received copies of the GNU General Public License and the
GNU Lesser General Public License along with the GNU MP Library.  If not,
see https://www.gnu.org/licenses/.



The code in this directory makes use of vector instructions added with z13.
These vector instructions are well-designed and naturally much more modern than
the legacy of S/390 instructions.  From GMP's perspective, the full set of
128-bit addition and subtraction, with register-based carry in and out, are
very useful.  Unfortunately, the multiply support is unexpectedly limited,
forcing GMP to use the the legacy mlg/mlgr instructions, juggling results
between plain and vector registers.


Torbj√∂rn has faster mul_2 and addmul_2, running at close to 2 cycles/limb on
z15.  That's not a whole lot faster than mul_1 and addmul_1, and as
sqr_basecase and mul_basecase are based on the _1 variants, we have not
committed the faster _2 code.

Here is how a new mul_basecase should be organised:

  1. If the rp pointer is 128-bit aligned, start with mul_2 to keep alignment.
     Else start with mul_1.  Now rp will be 128-bit aligned.

  2. Loop over addmul_2.  Probably don't expand it into 4 variants (addmul_2 is
     4-way unrolled) as that practice pays off less with the fewer outer loop
     iterations which are the result of using addmul_2.  Instead, do the n mod
     4 handling before each run.

  3. If there is now anything to do, finish off with an addmul_1.

This means that we will sometimes both do a mul_1 first and an addmul_1 last,
even if bn mod 2 == 0.  It is expected that that will be beneficial,
considering the alignment penalty for the 128-operations and the fact that the
_2 functions are not dramatically faster then the _1 functions.

A new sqr_basecase should use addmul_2 too.  Here, we might get significant
improvements as the branch predictor performs abysmally given the structure of
sqr_basecase; an addmul_2 based variant cuts the number of branches in half.
